{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams in Natural Language Processing\n",
    "\n",
    "#### Definition of n-grams\n",
    "\n",
    "    - In natural language processing (NLP) and computational linguistics,\n",
    "    - **n-grams** are contiguous sequences of n items (typically words or characters) extracted from a given text.\n",
    "    - They are used to analyze text data and can help in various applications such as language modeling, text classification, and machine translation.\n",
    "\n",
    "#### Types of n-grams\n",
    "\n",
    "1. **Unigrams (1-grams):** These are single items, such as individual words or characters. For example, in the sentence \"I love cats,\" the unigrams are:\n",
    "   - \"I\"\n",
    "   - \"love\"\n",
    "   - \"cats\"\n",
    "\n",
    "2. **Bigrams (2-grams):** These are sequences of two items. For the same sentence, the bigrams would be:\n",
    "   - \"I love\"\n",
    "   - \"love cats\"\n",
    "\n",
    "3. **Trigrams (3-grams):** These are sequences of three items. In this case, the trigram is:\n",
    "   - \"I love cats\"\n",
    "\n",
    "4. **N-grams (n-grams):** This generalizes the concept to any integer n, where n can be any positive integer. For example, if n=4, you would get 4-grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of n-grams\n",
    "\n",
    "1. **Language Modeling:** N-grams are often used in statistical language models to predict the likelihood of a sequence of words. For instance, a bigram model would predict the next word based on the previous word.\n",
    "\n",
    "2. **Text Classification:** N-grams can be used as features in machine learning algorithms for tasks like sentiment analysis or spam detection. For example, identifying frequent bigrams can help distinguish between different categories of text.\n",
    "\n",
    "3. **Search Engines:** N-grams improve search engines' ability to understand and match queries by considering multiple words in sequence.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "1. **Sparsity:** As n increases, the number of possible n-grams grows exponentially, leading to sparse data where many n-grams may not appear in the training data.\n",
    "\n",
    "2. **Context:** N-grams do not capture long-range dependencies well. For instance, a trigram model may miss the context of words that are farther apart in a sentence.\n",
    "\n",
    "3. **Dimensionality:** The feature space can become very large with higher n, making computations more resource-intensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FirstsVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
